\input{../base.tex}
\fancyhead[L]{Übungsblatt 2}
\begin{document}
\section*{Aufgabe 6}
Die Quelle $Q$ gibt die Zeichen 0 und 1 mit den Wahrscheinlichkeiten $p_Q(0) = \frac{1}{4}$ und $p_Q(1) = \frac{3}{4}$ aus.
Die Zeichen werden über einen binär symmetrischen Kanal mit $p = \frac{1}{16}$ übertragen.
\medskip
\begin{myList}

# Berechnung der Entropie der Quelle:
\begin{align*} 
H(Q) 
  &= \sum_{a\in A} p_Q(a)\cdot\log_2\tuple{\frac 1 {p_Q(a)}} \\
  &= \frac 1 4\log(4) + \frac 3 4\log\tuple{\frac 4 3} \\
  &\approx 0.5+0.311=0.811
\end{align*}

# Berechnung der Outputentropie:
\begin{align*}
p_E(0) &= \frac 1 4\frac{15}{16} + \frac 3 4\frac 1 {16} = \frac 9{32}\\
p_E(1) &= 1 - p_E(0) = \frac{23}{32}\\
H(E) &= \frac{9}{32}\log\tuple{\frac{32}{9}} + \frac{23}{32}\log\tuple{\frac{32}{23}}\\
&\approx 0.857
\end{align*}

# Berechnung der Streuentropie:
\begin{align*}
H(E|Q)&=H_p&&\text{(Skript 2.16)}\\
&= \frac 1 {16}\log(16) + \frac{15}{16}\log\tuple{\frac{16}{15}}\\
&\approx 0.337
\end{align*}

# Berechnung der mittleren Transinformation:
$$I(Q, E) = H(E) - H(E|Q) \approx 0.857 - 0.337 = 0.520$$ 

\end{myList}

\section*{Aufgabe 7}
\begin{myList}
#
$A = \lbrace 0,1 \rbrace$, $B = \lbrace 0,1,\ast \rbrace$
\begin{align*}
	p(0|0) &= \frac{1}{2} & p(1|0) &= \frac{1}{4} & p(\ast|0) &= \frac{1}{4} \\
	p(1|1) &= \frac{1}{2} & p(0|1) &= \frac{1}{4} & p(\ast|1) &= \frac{1}{4} 
\end{align*}

Berechne $I(Q,E)$ in Abhängigkeit von $p_Q$:\\
Sei $p_Q(0) = p, p_Q(1) = 1-p$.

Für die Outputwahrscheinlichkeiten gilt:
\begin{align*}
	p_E(0) &= p_Q(0) \cdot p(0|0) + p_Q(1) \cdot p(0|1)\\
	&= p \cdot \frac{1}{2} + (1-p) \cdot \frac{1}{4}  = \frac{p}{4} + \frac{1}{4} = \frac{1 + p}{4}\\
	p_E(1) &= p_Q(0) \cdot p(1|0) + p_Q(1) \cdot p(1|1)\\
	&= p \cdot \frac{1}{4} + (1-p) \cdot \frac{1}{2} = -\frac{p}{4} + \frac{1}{2} = \frac{2 - p}{4}\\
	p_E(\ast) &= p_Q(0) \cdot p(\ast|0) + p_Q(1) \cdot p(\ast|1)\\
	&= p \cdot \frac{1}{4} + (1-p) \cdot \frac{1}{4} = \frac{1}{4}
\end{align*}

Für die Outputentropie gilt:
\begin{align*}
	H(E) &= \sum\limits_{b \in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)}\right) \\
	&= \frac{1+p}{4}\cdot \log \left( \frac{4}{1+p}\right) + \frac{2-p}{4} \cdot \log \left( \frac{4}{2-p} \right) + \frac{1}{4}\cdot \log 4 \\
	&= \frac{8 - (2-p)\log(2-p) - (1+p)\log(1+p)}{4}
\end{align*}

Für die einzelnen $H(E|a)$ gilt:
\begin{align*}
	H(E|0) &= \sum\limits_{b\in B} p(b|0) \cdot \log \left(\frac{1}{p(b|0)} \right)\\
	&= p(0|0) \cdot \log \left(\frac{1}{p(0|0)} \right) + p(1|0) \cdot \log \left(\frac{1}{p(1|0)} \right) + p(\ast|0) \cdot \log \left(\frac{1}{p(\ast|0)} \right)\\
	&= \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 + \frac{1}{4} \cdot 2 = \frac{3}{2}\\
	H(E|1) &= \sum\limits_{b\in B} p(b|1) \cdot \log \left(\frac{1}{p(b|1)} \right) = \frac{3}{2} \text{ (analog)}
\end{align*}

Für die Streuentropie gilt:
\begin{align*}
	H(E|Q) &= \sum\limits_{a \in A} p_Q(a) \cdot H(E|a) \\
	&= p \cdot \frac{3}{2} + (1-p) \cdot \frac{3}{2} = \frac{3}{2}
\end{align*}

Für die mittlere Transinformation gilt:
\begin{align*}
	I(Q|E) &= H(E) - H(E|Q)\\
	&= \frac{2 -(2-p)\log(2-p) - (1+p)\log(1+p)}{4}
\end{align*}

Maximiere die Funktion:\\
Es gilt:
\begin{equation*}
	\frac{dI(Q|E)}{dp} = \frac{\log(2-p)- \log(p+1)}{4}
\end{equation*}
Der Scheitelpunkt der Kurve wird durch Nullsetzen bestimmt:
\begin{align*}
	&\log(2-p) - \log(p+1) = 0 \\
	&\Leftrightarrow \log(2-p) = \log(p+1) \\
	&\Leftrightarrow 2-p = p+1 \\
	&\Leftrightarrow p = 0.5
\end{align*}
Durch einsetzen in die Formel für $I(Q|E)$ erhält man $\approx 0.0612$.
Also ist die Kapazität des Kanals $\approx 0.0612$.
#
$A = B = \lbrace 0,1,2 \rbrace$
\begin{align*}
	p(0|0) &= p & p(1|0) &= (1-p) & p(2|0) &= 0 \\
	p(1|1) &= p & p(0|1) &= (1-p) & p(2|1) &= 0 \\
	p(2|2) &= 1 & p(0|2) &= 0 & p(1|2) &= 0
\end{align*}

Berechne $I(Q,E)$ in Abhängigkeit von $p_Q$:\\
Für die Outputwahrscheinlichkeiten gilt:
\begin{align*}
	p_E(0) &= p_Q(0) \cdot p(0|0) + p_Q(1) \cdot p(0|1) + p_Q(2) \cdot p(0|2) \\
	&= p_Q(0) \cdot p + p_Q(1) \cdot (1-p) \\
	p_E(1) &= p_Q(0) \cdot p(1|0) + p_Q(1) \cdot p(1|1) + p_Q(2) \cdot p(1|2)\\
	&= p_Q(0) \cdot (1-p) + p_Q(1) \cdot p \\
	p_E(2) &= p_Q(0) \cdot p(2|0) + p_Q(1) \cdot p(2|1) + p_Q(2) \cdot p(2|2)\\
	&= p_Q(2)
\end{align*}

Sei im folgenden $p_Q(0) = a, p_Q(1) = b, p_Q(2) = c$, dann gilt für die Outputentropie:
\begin{align*}
	H(E) &= \sum\limits_{b \in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)} \right)\\
	&= (ap + b(1-p))\log \left(\frac{1}{ap + b(1-p)}\right) + (bp + a(1-p))\log \left(\frac{1}{bp + a(1-p)}\right)\\
	&+ c \log\left( \frac{1}{c} \right)
\end{align*}

Für die einzelnen $H(E|a)$ gilt:
\begin{align*}
	H(E|0) &= p(0|0) \cdot \log \left(\frac{1}{p(0|0)} \right) + p(1|0) \cdot \log \left(\frac{1}{p(1|0)} \right) + p(2|0) \cdot \log \left(\frac{1}{p(2|0)} \right)\\
	&= p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}\\
	H(E|1) &= p(0|1) \cdot \log \left(\frac{1}{p(0|1)} \right) + p(1|1) \cdot \log \left(\frac{1}{p(1|1)} \right) + p(2|1) \cdot \log \left(\frac{1}{p(2|1)} \right)\\
	&= p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}\\
	H(E|2) &= p(0|2) \cdot \log \left(\frac{1}{p(0|2)} \right) + p(1|2) \cdot \log \left(\frac{1}{p(1|2)} \right) + p(2|2) \cdot \log \left(\frac{1}{p(2|2)} \right) \\
	&= 0
\end{align*}

Für die Streuentropie gilt:
\begin{align*}
	H(E|Q) &= a \cdot (p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}) + b \cdot (p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}) \\
	&= (a+b) \cdot (p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p})
\end{align*}

Maximiere die Funktion:\\
TODO
\end{myList}

\section*{Aufgabe 8}
\begin{myList}
#
\begin{align*}
	H(Q) &= \sum\limits_{a \in A} p_Q(a) \cdot \log \left( \frac{1}{p_Q(a)} \right) \\
	H(E|Q) &= \sum\limits_{a \in A} \sum\limits_{b \in B} p_Q(a) \cdot p(b|a) \cdot \log \left( \frac{1}{p(b|a)}\right) 
\end{align*}

Zeige $H(E|Q) = H(Q,E) - H(Q)$:
\begin{align*}
	H(Q,E) &= \sum\limits_{(a,b) \in A\times B} p(a,b) \cdot \log\left(\frac{1}{p(a,b)}\right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left( \frac{1}{p(b|a) \cdot p_Q(a)} \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} -p(b|a) \cdot p_Q(a) \cdot \log \left(p(b|a) \cdot p_Q(a) \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p(b|a)} \right) + p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p(b|a)} \right) + \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) \\
	&= H(E|Q) + \sum\limits_{a \in A}  p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) = H(E|Q) + H(Q)\\
	&\Rightarrow H(E|Q) = H(Q,E) - H(Q)
\end{align*}

Zeige $H(E|Q) \leq H(E)$:

\begin{align*}
	 H(E|Q) &\leq H(E) \\
	\Leftrightarrow H(Q,E) - H(Q) &\leq H(E) \\
	\Leftrightarrow H(Q,E) &\leq H(Q) + H(E) \text{(gilt nach Vorlesung 2.11.b)}
\end{align*}

#
\begin{align*}
	H(Q|E) &= \sum\limits_{(a.b) \in A \times B} p_E(b)\cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)}\right)
\end{align*}

Zeige $H(Q|E) = H(Q,E) - H(E)$
\begin{align*}
	H(Q|E) &= \sum\limits_{(a.b) \in A \times B} p_E(b)\cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)}\right) \\
	&=\sum\limits_{a \in A} \sum\limits_{b \in B}  p(a,b) \cdot \log \left( \frac{p_E(b)}{p(a,b)}\right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(a,b) \cdot \log (p_E(b)) - p(a,b) \cdot \log(p(a,b)) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(a,b) \cdot \log\left(\frac{1}{p(a,b)}\right) - \sum\limits_{a \in A} \sum\limits_{b \in B}  p(a,b) \cdot \log \left(\frac{1}{p_E(b)}\right) \\
	&= H(Q,E) - \sum\limits_{b \in B} \sum\limits_{a \in A}  p(a|b) \cdot p_E(b) \cdot \log \left(\frac{1}{p_E(b)}\right) \\
	&= H(Q,E) - \sum\limits_{b \in B} p_E(b) \cdot \log \left(\frac{1}{p_E(b)}\right) = H(Q,E) - H(E)
\end{align*}

#
Zeige $I(Q,E) = H(Q) - H(Q|E) \leq H(Q)$

$H(Q) - H(Q|E) \leq H(Q)$ gilt trivialerweise da $H(Q|E) \geq 0$.\\
$H(Q|E) \geq 0$ gilt da sowohl $p_E(b), p(a|b)$ als auch $\log\left( \frac{1}{p(a|b) }\right)$ größer oder gleich 0 sind.

Zeige $I(Q,E) = H(Q) - H(Q|E)$
\begin{align*}
	I(Q,E) &=_{Def} H(E) - H(E|Q) \\
	&= H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_Q(a) \cdot p(b|a) \cdot \log \left( \frac{1}{p(b|a)}\right) \\
	&\underset{Bayes}{=}  H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{p_Q(a)}{p(a|b) \cdot p_E(b)} \right)\\
	&=  H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)} \right)
	-
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p_E(b)} \right)
	\\	 
	&+
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p_Q(a)} \right)\\
	&= H(E) - H(Q|E) - \sum\limits_{b\in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)} \right) + \sum\limits_{a \in A} p_Q(a) \cdot \log \left( \frac{1}{p_Q(a)} \right)\\
	&= H(E) - H(Q|E) - H(E) + H(Q) \\
	&= H(Q) - H(Q|E)
\end{align*}
\end{myList}

\section*{Aufgabe 9}

Sei die Codierungsfunktion $c: \mathbb{Z}^3_2 \rightarrow \mathbb{Z}^5_2 , (x,y,z) \mapsto (x,y,z,x+z,y+z)$.\\
Sei $\C = \lbrace v \in \mathbb{Z}^5_2 | \exists w \in \mathbb{Z}^3_2: v = c(w) \rbrace$.
\begin{myList}
# Zeigen Sie, dass $\C$ ein linearer Code ist:\\
$\C$ ist ein linearer Code gdw. $\forall a, b\in\C: x+y\in\C$.\\
Seien $a = (x, y, z, w=x+z, v=y+z), b=(x', y', z', w'=x'+z', v'=y'+z')\in\C$ beliebig. Dann gilt:
$$a+b = (x'' = x+x', y'' = y+y', z'' = z+z', w'' = w+w', v'' = v+v')$$
$a+b$ ist genau dann in $\C$, wenn gilt:
\begin{align*}
w'' &= x''+z''\\
v'' &= y''+z''
\end{align*}
Wir überprüfen einzeln:
\begin{align*}
w'' &= w+w' = x+z+x'+z'=(x+x')+(z+z') = x'' + z''\\
v'' &= v+v' = y+z+y'+z'=(y+y')+(z+z') = y'' + z''\\
\end{align*}
Also ist $\C$ ein linearer Code.
# Geben Sie eine Erzeugermatrix von $\C$ an:\\
$$\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
1 & 0 & 1\\
0 & 1 & 1
\end{pmatrix}$$

#
Bestimmen Sie die Länge $n$, die Dimension $k$ und den Minimalabstand $d$ von $\C$:\\
Länge $n = 5$\\
Dimension $k = 3$\\
Minimalabstand $d$:\\
Alle Codewörter:
$$\begin{array}{cc}
\multicolumn{2}{c}{\text{Codewörter}}\\
001 & 11 \\
010 & 01 \\
011 & 10 \\
100 & 10 \\
101 & 01 \\
110 & 11 \\
111 & 00 \\
\end{array}$$
$d=\min\set{3, 2, 3, 2, 3, 4, 3, 4, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3} = 2$ 
# Wieviele Fehler kann $\C$ erkennen / korrigieren?\\
$\C$ kann 1 Fehler erkennen und keinen sicher korrigieren.

TODO
\end{myList}

\section*{Aufgabe 10}
\begin{myList}
#
Schreiben Sie ein Programm, das zufällig einen 4-dimensionalen Untervektorraum des $\mathbb{Z}^10_2$ erzeugt und den Minimalabstand von $U$ berechnet.
Welchen durchschnittlichen Minimalabstand erhalten Sie nach 100 Programmdurchläufen?\\
Im Schnitt ist der Minimalabstand 2.9987 (über 10000 Iterationen).

TODO

#
Konstruieren Sie einen binären $[10,4]$-Code mit möglichst großen Minimalabstand.\\
Bester von uns gefundener Code:\\
\begin{align*}
\C = \tuple{\begin{array}{ccc}
0101&0110&01\\
0011&1001&01\\
1100&1000&00\\
1011&0010&10
\end{array}},~~d(\C)=6
\end{align*}

TODO
\end{myList}\end{document}





