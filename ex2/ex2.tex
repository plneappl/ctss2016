\input{../base.tex}
\fancyhead[L]{Übungsblatt 2}
\begin{document}
\section*{Aufgabe 6}
Die Quelle $Q$ gibt die Zeichen 0 und 1 mit den Wahrscheinlichkeiten $p_Q(0) = \frac{1}{4}$ und $p_Q(1) = \frac{3}{4}$ aus.
Die Zeichen werden über einen binär symmetrischen Kanal mit $p = \frac{1}{16}$ übertragen.
\medskip
\begin{myList}

# Berechnung der Entropie der Quelle:
\begin{align*} 
H(Q) 
  &= \sum_{a\in A} p_Q(a)\cdot\log_2\tuple{\frac 1 {p_Q(a)}} \\
  &= \frac 1 4\log(4) + \frac 3 4\log\tuple{\frac 4 3} \\
  &\approx 0.5+0.311=0.811
\end{align*}

# Berechnung der Outputentropie:
\begin{align*}
p_E(0) &= \frac 1 4\frac{15}{16} + \frac 3 4\frac 1 {16} = \frac 9{32}\\
p_E(1) &= 1 - p_E(0) = \frac{23}{32}\\
H(E) &= \frac{9}{32}\log\tuple{\frac{32}{9}} + \frac{23}{32}\log\tuple{\frac{32}{23}}\\
&\approx 0.857
\end{align*}

# Berechnung der Streuentropie:
\begin{align*}
H(E|Q)&=H_p&&\text{(Skript 2.16)}\\
&= \frac 1 {16}\log(16) + \frac{15}{16}\log\tuple{\frac{16}{15}}\\
&\approx 0.337
\end{align*}

# Berechnung der mittleren Transinformation:
$$I(Q, E) = H(E) - H(E|Q) \approx 0.857 - 0.337 = 0.520$$ 

\end{myList}

\section*{Aufgabe 7}
\begin{myList}
#
$A = \lbrace 0,1 \rbrace$, $B = \lbrace 0,1,\ast \rbrace$
\begin{align*}
	p(0|0) &= \frac{1}{2} & p(1|0) &= \frac{1}{4} & p(\ast|0) &= \frac{1}{4} \\
	p(1|1) &= \frac{1}{2} & p(0|1) &= \frac{1}{4} & p(\ast|1) &= \frac{1}{4} 
\end{align*}

Berechne $I(Q,E)$ in Abhängigkeit von $p_Q$:\\
Sei $p_Q(0) = p, p_Q(1) = 1-p$.

Für die Outputwahrscheinlichkeiten gilt:
\begin{align*}
	p_E(0) &= p_Q(0) \cdot p(0|0) + p_Q(1) \cdot p(0|1)\\
	&= p \cdot \frac{1}{2} + (1-p) \cdot \frac{1}{4}  = \frac{p}{4} + \frac{1}{4} = \frac{1 + p}{4}\\
	p_E(1) &= p_Q(0) \cdot p(1|0) + p_Q(1) \cdot p(1|1)\\
	&= p \cdot \frac{1}{4} + (1-p) \cdot \frac{1}{2} = -\frac{p}{4} + \frac{1}{2} = \frac{2 - p}{4}\\
	p_E(\ast) &= p_Q(0) \cdot p(\ast|0) + p_Q(1) \cdot p(\ast|1)\\
	&= p \cdot \frac{1}{4} + (1-p) \cdot \frac{1}{4} = \frac{1}{4}
\end{align*}

Für die Outputentropie gilt:
\begin{align*}
	H(E) &= \sum\limits_{b \in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)}\right) \\
	&= \frac{1+p}{4}\cdot \log \left( \frac{4}{1+p}\right) + \frac{2-p}{4} \cdot \log \left( \frac{4}{2-p} \right) + \frac{1}{4}\cdot \log 4 \\
	&= \frac{8 - (2-p)\log(2-p) - (1+p)\log(1+p)}{4}
\end{align*}

Für die einzelnen $H(E|a)$ gilt:
\begin{align*}
	H(E|0) &= \sum\limits_{b\in B} p(b|0) \cdot \log \left(\frac{1}{p(b|0)} \right)\\
	&= p(0|0) \cdot \log \left(\frac{1}{p(0|0)} \right) + p(1|0) \cdot \log \left(\frac{1}{p(1|0)} \right) + p(\ast|0) \cdot \log \left(\frac{1}{p(\ast|0)} \right)\\
	&= \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 + \frac{1}{4} \cdot 2 = \frac{3}{2}\\
	H(E|1) &= \sum\limits_{b\in B} p(b|1) \cdot \log \left(\frac{1}{p(b|1)} \right) = \frac{3}{2} \text{ (analog)}
\end{align*}

Für die Streuentropie gilt:
\begin{align*}
	H(E|Q) &= \sum\limits_{a \in A} p_Q(a) \cdot H(E|a) \\
	&= p \cdot \frac{3}{2} + (1-p) \cdot \frac{3}{2} = \frac{3}{2}
\end{align*}

Für die mittlere Transinformation gilt:
\begin{align*}
	I(Q|E) &= H(E) - H(E|Q)\\
	&= \frac{2 -(2-p)\log(2-p) - (1+p)\log(1+p)}{4}
\end{align*}

Maximiere die Funktion:\\
Es gilt:
\begin{equation*}
	\frac{dI(Q|E)}{dp} = \frac{\log(2-p)- \log(p+1)}{4}
\end{equation*}
Der Scheitelpunkt der Kurve wird durch Nullsetzen bestimmt:
\begin{align*}
	&\log(2-p) - \log(p+1) = 0 \\
	&\Leftrightarrow \log(2-p) = \log(p+1) \\
	&\Leftrightarrow 2-p = p+1 \\
	&\Leftrightarrow p = 0.5
\end{align*}
Durch einsetzen in die Formel für $I(Q|E)$ erhält man $\approx 0.0612$.
Also ist die Kapazität des Kanals $\approx 0.0612$.

#
$A = B = \lbrace 0,1,2 \rbrace$
\begin{align*}
	p(0|0) &= p & p(1|0) &= (1-p) & p(2|0) &= 0 \\
	p(1|1) &= p & p(0|1) &= (1-p) & p(2|1) &= 0 \\
	p(2|2) &= 1 & p(0|2) &= 0 & p(1|2) &= 0
\end{align*}

Berechne $I(Q,E)$ in Abhängigkeit von $p_Q$:\\
Für die Outputwahrscheinlichkeiten gilt:
\begin{align*}
	p_E(0) &= p_Q(0) \cdot p(0|0) + p_Q(1) \cdot p(0|1) + p_Q(2) \cdot p(0|2) \\
	&= p_Q(0) \cdot p + p_Q(1) \cdot (1-p) \\
	p_E(1) &= p_Q(0) \cdot p(1|0) + p_Q(1) \cdot p(1|1) + p_Q(2) \cdot p(1|2)\\
	&= p_Q(0) \cdot (1-p) + p_Q(1) \cdot p \\
	p_E(2) &= p_Q(0) \cdot p(2|0) + p_Q(1) \cdot p(2|1) + p_Q(2) \cdot p(2|2)\\
	&= p_Q(2)
\end{align*}

Sei im folgenden $p_Q(0) = a, p_Q(1) = b, p_Q(2) = c$, dann gilt für die Outputentropie:
\begin{align*}
	H(E) &= \sum\limits_{b \in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)} \right)\\
	&= (ap + b(1-p))\log \left(\frac{1}{ap + b(1-p)}\right) + (bp + a(1-p))\log \left(\frac{1}{bp + a(1-p)}\right)\\
	&+ c \log\left( \frac{1}{c} \right) \\
	&= -(ap + b(1-p)\log(ap + b(1-p) - (bp + a(1-p))\log(bp + a(1-p)) - c\log(c) \\
	&= -(ap + b - bp)\log(ap + b - bp) -(bp + a - ap)\log(bp + a - ap) - c\log(c)
\end{align*}


Für die einzelnen $H(E|a)$ gilt:
\begin{align*}
	H(E|0) &= p(0|0) \cdot \log \left(\frac{1}{p(0|0)} \right) + p(1|0) \cdot \log \left(\frac{1}{p(1|0)} \right) + p(2|0) \cdot \log \left(\frac{1}{p(2|0)} \right)\\
	&= p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p} = -p\log(p) - (1-p)\log(1-p)\\
	H(E|1) &= p(0|1) \cdot \log \left(\frac{1}{p(0|1)} \right) + p(1|1) \cdot \log \left(\frac{1}{p(1|1)} \right) + p(2|1) \cdot \log \left(\frac{1}{p(2|1)} \right)\\
	&= p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}= -p\log(p) - (1-p)\log(1-p)\\
	H(E|2) &= p(0|2) \cdot \log \left(\frac{1}{p(0|2)} \right) + p(1|2) \cdot \log \left(\frac{1}{p(1|2)} \right) + p(2|2) \cdot \log \left(\frac{1}{p(2|2)} \right) \\
	&= 0
\end{align*}

Für die Streuentropie gilt:
\begin{align*}
	H(E|Q) &= a \cdot \left(p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p}\right) + b \cdot \left(p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p} \right) \\
	&= (a+b) \cdot \left(p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p} \right)
\end{align*}

Aus dem Faktor $(a+b)$ ist ersichtlich dass das Verhältnis von $a$ und $b$ keinen Einfluss auf den Wert der Streuentropie hat, sondern nur deren Summe.
Dadurch kann eine Vereinfachung erreicht werden, da man zeigen kann dass die mittlere Transinformation nur dann maximal werden kann falls gilt $a = b$.
Der Anteil an der Outputentropie von $a$ und $b$, i.e.  $-(ap + b - bp)\log(ap + b - bp) -(bp + a - ap)\log-(bp + a - ap)$ ist maximal bei $a = b$.
(In Analogie zu dem Beweis von Satz 2.22 unter dem Constraint $a + b = 1-c$ anstatt $a +b = 1$).

$H(E)$ kann also durch folgendes $H'(E)$ ersetzt werden:
\begin{align*}
	H'(E) &= -(ap + a - ap)\log(ap + a - ap) -(ap + a - ap)\log(ap + a - ap) + c \log \left(\frac{1}{c} \right)\\
	&= -2a\log(a) + c \log \left(\frac{1}{c} \right) = 2 a\log \left( \frac{1}{a}\right) + c \log \left(\frac{1}{c} \right) \\
	&= 2 a\log \left( \frac{1}{a}\right) + (1-2a) \log \left(\frac{1}{1 - 2a} \right)
\end{align*}

Für die mittlere Transinformation gilt:
\begin{align*}
	I(Q|E) &= H'(E) - H(E|Q) \\
	&= 2 a\log \left( \frac{1}{a}\right) + (1-2a) \log \left(\frac{1}{1 - 2a} \right) - 2a\left(p\log \frac{1}{p} + (1-p) \log \frac{1}{1-p} \right)
\end{align*}

Maximiere die Funktion:\\
Bestimme die partielle Ableitung von $I(Q|E)$ von $a$:
\begin{align*}
	\frac{\partial I(Q|E)}{\partial a} &= 2 \cdot \left( \log \frac{1}{a} - \log(\frac{1}{1-2a}) + p\log \frac{1}{1-p} - \log \frac{1}{1-p} - p \log \frac{1}{p} \right)
\end{align*}

Setze die Gleichung gleich $0$ und löse nach $a$ auf:
\begin{align*}
	a = \frac{1}{p^{-p} \cdot (1-p)^{p-1} + 2}
\end{align*}
Wenn man diesen Wert für $a$ in die Gleichung für $I(Q|E)$ einsetzt so erhält man die Kapazität des Kanals in Abhängigkeit von $p$.
($b$ und $c$ ergeben sich aus $a$).

Als \enquote{sanity check} überprüfen wir die Lösung für die beiden Werte $p = 1$ und $p = 0.5$.
\begin{align*}
	p &= 1  &\Rightarrow a &= \frac{1}{3} &\Rightarrow I(Q|E) &= \log(3) \\
	p &= 0.5 &\Rightarrow a &= 0.25 &\Rightarrow I(Q|E) &= 1
\end{align*}
Diese Ergebnisse sind intuitiv richtig, da für $p = 1$ der Kanal einem ungestörten Kanal mit 3 Eingabe und Ausgabesymbolen entspricht, und falls $p = \frac{1}{2}$, so kann der Kanal wie ein ungestörter Kanal mit 2 Eingabe und Ausgabesymbolen genutzt werden.
(Dabei wird der total gestörte Teil des Kanals wie ein Symbol betrachtet).
\end{myList}

\section*{Aufgabe 8}
\begin{myList}
#
\begin{align*}
	H(Q) &= \sum\limits_{a \in A} p_Q(a) \cdot \log \left( \frac{1}{p_Q(a)} \right) \\
	H(E|Q) &= \sum\limits_{a \in A} \sum\limits_{b \in B} p_Q(a) \cdot p(b|a) \cdot \log \left( \frac{1}{p(b|a)}\right) 
\end{align*}

Zeige $H(E|Q) = H(Q,E) - H(Q)$:
\begin{align*}
	H(Q,E) &= \sum\limits_{(a,b) \in A\times B} p(a,b) \cdot \log\left(\frac{1}{p(a,b)}\right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left( \frac{1}{p(b|a) \cdot p_Q(a)} \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} -p(b|a) \cdot p_Q(a) \cdot \log \left(p(b|a) \cdot p_Q(a) \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p(b|a)} \right) + p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p(b|a)} \right) + \sum\limits_{a \in A} \sum\limits_{b \in B} p(b|a) \cdot p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) \\
	&= H(E|Q) + \sum\limits_{a \in A}  p_Q(a) \cdot \log \left(\frac{1}{p_Q(a)} \right) = H(E|Q) + H(Q)\\
	&\Rightarrow H(E|Q) = H(Q,E) - H(Q)
\end{align*}

Zeige $H(E|Q) \leq H(E)$:

\begin{align*}
	 H(E|Q) &\leq H(E) \\
	\Leftrightarrow H(Q,E) - H(Q) &\leq H(E) \\
	\Leftrightarrow H(Q,E) &\leq H(Q) + H(E) \text{(gilt nach Vorlesung 2.11.b)}
\end{align*}

#
\begin{align*}
	H(Q|E) &= \sum\limits_{(a.b) \in A \times B} p_E(b)\cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)}\right)
\end{align*}

Zeige $H(Q|E) = H(Q,E) - H(E)$
\begin{align*}
	H(Q|E) &= \sum\limits_{(a.b) \in A \times B} p_E(b)\cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)}\right) \\
	&=\sum\limits_{a \in A} \sum\limits_{b \in B}  p(a,b) \cdot \log \left( \frac{p_E(b)}{p(a,b)}\right) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(a,b) \cdot \log (p_E(b)) - p(a,b) \cdot \log(p(a,b)) \\
	&= \sum\limits_{a \in A} \sum\limits_{b \in B} p(a,b) \cdot \log\left(\frac{1}{p(a,b)}\right) - \sum\limits_{a \in A} \sum\limits_{b \in B}  p(a,b) \cdot \log \left(\frac{1}{p_E(b)}\right) \\
	&= H(Q,E) - \sum\limits_{b \in B} \sum\limits_{a \in A}  p(a|b) \cdot p_E(b) \cdot \log \left(\frac{1}{p_E(b)}\right) \\
	&= H(Q,E) - \sum\limits_{b \in B} p_E(b) \cdot \log \left(\frac{1}{p_E(b)}\right) = H(Q,E) - H(E)
\end{align*}

#
Zeige $I(Q,E) = H(Q) - H(Q|E) \leq H(Q)$

$H(Q) - H(Q|E) \leq H(Q)$ gilt trivialerweise da $H(Q|E) \geq 0$.\\
$H(Q|E) \geq 0$ gilt da sowohl $p_E(b), p(a|b)$ als auch $\log\left( \frac{1}{p(a|b) }\right)$ größer oder gleich 0 sind.

Zeige $I(Q,E) = H(Q) - H(Q|E)$
\begin{align*}
	I(Q,E) &=_{Def} H(E) - H(E|Q) \\
	&= H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_Q(a) \cdot p(b|a) \cdot \log \left( \frac{1}{p(b|a)}\right) \\
	&\underset{Bayes}{=}  H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{p_Q(a)}{p(a|b) \cdot p_E(b)} \right)\\
	&=  H(E) -
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p(a|b)} \right)
	-
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p_E(b)} \right)
	\\	 
	&+
	\sum\limits_{a\in A}\sum\limits_{b \in B} p_E(b) \cdot p(a|b) \cdot \log \left( \frac{1}{p_Q(a)} \right)\\
	&= H(E) - H(Q|E) - \sum\limits_{b\in B} p_E(b) \cdot \log \left( \frac{1}{p_E(b)} \right) + \sum\limits_{a \in A} p_Q(a) \cdot \log \left( \frac{1}{p_Q(a)} \right)\\
	&= H(E) - H(Q|E) - H(E) + H(Q) \\
	&= H(Q) - H(Q|E)
\end{align*}
\end{myList}

\section*{Aufgabe 9}

Sei die Codierungsfunktion $c: \mathbb{Z}^3_2 \rightarrow \mathbb{Z}^5_2 , (x,y,z) \mapsto (x,y,z,x+z,y+z)$.\\
Sei $\C = \lbrace v \in \mathbb{Z}^5_2 | \exists w \in \mathbb{Z}^3_2: v = c(w) \rbrace$.
\begin{myList}
# Zeigen Sie, dass $\C$ ein linearer Code ist:\\
$\C$ ist ein linearer Code gdw. $\forall a, b\in\C: x+y\in\C$.\\
Seien $a = (x, y, z, w=x+z, v=y+z), b=(x', y', z', w'=x'+z', v'=y'+z')\in\C$ beliebig. Dann gilt:
$$a+b = (x'' = x+x', y'' = y+y', z'' = z+z', w'' = w+w', v'' = v+v')$$
$a+b$ ist genau dann in $\C$, wenn gilt:
\begin{align*}
w'' &= x''+z''\\
v'' &= y''+z''
\end{align*}
Wir überprüfen einzeln:
\begin{align*}
w'' &= w+w' = x+z+x'+z'=(x+x')+(z+z') = x'' + z''\\
v'' &= v+v' = y+z+y'+z'=(y+y')+(z+z') = y'' + z''\\
\end{align*}
Also ist $\C$ ein linearer Code.
# Geben Sie eine Erzeugermatrix von $\C$ an:\\
$$\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
1 & 0 & 1\\
0 & 1 & 1
\end{pmatrix}$$

#
Bestimmen Sie die Länge $n$, die Dimension $k$ und den Minimalabstand $d$ von $\C$:\\
Länge $n = 5$\\
Dimension $k = 3$\\
Minimalabstand $d$:\\
Alle Codewörter:
$$\begin{array}{cc}
\multicolumn{2}{c}{\text{Codewörter}}\\
001 & 11 \\
010 & 01 \\
011 & 10 \\
100 & 10 \\
101 & 01 \\
110 & 11 \\
111 & 00 \\
\end{array}$$
$d=\min\set{3, 2, 3, 2, 3, 4, 3, 4, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3} = 2$ 
# Wieviele Fehler kann $\C$ erkennen / korrigieren?\\
$\C$ kann 1 Fehler erkennen und keinen sicher korrigieren.


\end{myList}

\section*{Aufgabe 10}
\begin{myList}
#
Schreiben Sie ein Programm, das zufällig einen 4-dimensionalen Untervektorraum des $\mathbb{Z}^10_2$ erzeugt und den Minimalabstand von $U$ berechnet.
Welchen durchschnittlichen Minimalabstand erhalten Sie nach 100 Programmdurchläufen?\\
Im Schnitt ist der Minimalabstand 2.9987 (über 10000 Iterationen).


#
Konstruieren Sie einen binären $[10,4]$-Code mit möglichst großen Minimalabstand.\\
Bester von uns gefundener Code:\\
\begin{align*}
\C = \tuple{\begin{array}{ccc}
0101&0110&01\\
0011&1001&01\\
1100&1000&00\\
1011&0010&10
\end{array}},~~d(\C)=6
\end{align*}

\end{myList}\end{document}





